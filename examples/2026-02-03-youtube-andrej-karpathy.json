{
    "id": "2026-02-03-youtube-andrej-karpathy",
    "source": "youtube",
    "title": "Andrej Karpathy: The Future of Neural Networks",
    "summary": "Karpathy diskutiert die nächste Generation von Neural Networks. Fokus auf Sparse Architectures, besseres Reasoning durch Chain-of-Thought, und warum kleinere Modelle mit besseren Daten oft besser sind.",
    "content": "## Kernpunkte\n\n### Sparse Architectures\n- Mixture of Experts (MoE) wird Standard\n- Nur 10-20% der Parameter aktiv pro Inference\n- Drastisch reduzierte Compute-Kosten\n\n### Chain-of-Thought Evolution\n- CoT wird in Training integriert, nicht nur Prompting\n- \"Thinking tokens\" als nativer Teil der Architektur\n- Ermöglicht echtes Multi-Step Reasoning\n\n### Data Quality > Model Size\n- \"Ein 7B Modell mit perfekten Daten schlägt ein 70B Modell mit mittelmäßigen Daten\"\n- Synthetic Data Generation wird kritisch\n- Curriculum Learning macht Comeback\n\n## Takeaways\n\n1. **Für Entwickler**: Fokus auf Data Pipelines, nicht nur Model Selection\n2. **Für Researcher**: Sparse Architectures sind das nächste große Ding\n3. **Für Unternehmen**: Kleinere, spezialisierte Modelle oft besser als große generische",
    "links": [
        "https://youtube.com/watch?v=abc123",
        "https://karpathy.ai",
        "https://arxiv.org/abs/2024.12345"
    ],
    "created_at": "2026-02-03T14:30:00Z",
    "metadata": {
        "video_url": "https://youtube.com/watch?v=abc123",
        "duration": "47:23",
        "channel": "Andrej Karpathy"
    }
}
